{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074753bd",
   "metadata": {},
   "source": [
    "# Project start\n",
    "\n",
    "I will use the openAI API. Specifically, the gpt-3.5-turbo-instruct model.\n",
    "\n",
    "You can change the template as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdef53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe503efc",
   "metadata": {},
   "source": [
    "You can create prompts and trigger your model however you like. Follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe9a43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of France is Paris.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d8222",
   "metadata": {},
   "source": [
    "### Using the stream() method\n",
    "\n",
    "You can use the string() method inside a for structure in python to have the model generate one sentence at a time. Follow these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54353c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Gentle waves caress\n",
      "Whispers of secrets untold\n",
      "Eternal embrace."
     ]
    }
   ],
   "source": [
    "prompt = \"Create a haiku about the ocean.\"\n",
    "for step in llm.stream(prompt):\n",
    "    print(step, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945a39c",
   "metadata": {},
   "source": [
    "Asking questions in batches to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae5497d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nThe capital of France is Paris.',\n",
       " '\\n\\nThe capital of Germany is Berlin.',\n",
       " '\\n\\nThe capital of Italy is Rome.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch processing\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the capital of Germany?\",\n",
    "    \"What is the capital of Italy?\"\n",
    "    ]\n",
    "# This will return a list of responses\n",
    "llm.batch(questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58984a4",
   "metadata": {},
   "source": [
    "### Chat models\n",
    "\n",
    "In LangChain, chat models are abstractions that provide a structured interface for interacting with conversational large language models (LLMs). Unlike traditional LLMs that operate on plain text inputs and outputs, chat models handle sequences of messages, each associated with a role (e.g., \"system\", \"user\", \"assistant\"), facilitating more natural and context-aware interactions ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af798f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f335eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an assistant who responds with irony\"),\n",
    "    HumanMessage(content=\"What is the role of cache memory?\")\n",
    "]\n",
    "\n",
    "answers = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7241754",
   "metadata": {},
   "source": [
    "The model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5996979f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, cache memory? It's just there to take up space and make your computer look fancy. Totally not important for speeding up data access or anything like that.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09486b3",
   "metadata": {},
   "source": [
    "Observations on metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29315c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 33,\n",
       "  'prompt_tokens': 27,\n",
       "  'total_tokens': 60,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac9f47",
   "metadata": {},
   "source": [
    "### Prompt few shots\n",
    "\n",
    "In LangChain (and more broadly in the context of using LLMs), \"prompt few-shot\" refers to a technique where examples of input-output pairs are included in the prompt to guide the language model in generating more accurate or relevant responses. This is a part of few-shot learning, where the model is shown a few examples before making a prediction or generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66bbc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c01960b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Saturday', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 53, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-096078c2-709d-45e7-b900-958c85339f4a-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"what is the first day of the week?\"),\n",
    "    AIMessage(content=\"sunday\"),\n",
    "    HumanMessage(content=\"what is the third day of the week?\"),\n",
    "    AIMessage(content=\"Tuesday\"),\n",
    "    HumanMessage(content=\"What is the last day of the week?\"),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ddb6e",
   "metadata": {},
   "source": [
    "### langchain debug\n",
    "\n",
    "You can use langchain debug to see what is happening at the low level of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1170f6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: what is the first day of the week?\\nAI: sunday\\nHuman: what is the third day of the week?\\nAI: Tuesday\\nHuman: What is the last day of the week?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOpenAI] [1.08s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Sunday\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Sunday\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 1,\n",
      "                \"prompt_tokens\": 53,\n",
      "                \"total_tokens\": 54,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-95eb2123-e4a1-409d-99c5-a43e2d09b9aa-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 1,\n",
      "      \"prompt_tokens\": 53,\n",
      "      \"total_tokens\": 54,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sunday', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 53, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-95eb2123-e4a1-409d-99c5-a43e2d09b9aa-0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dacb3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec9ca0",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "In LangChain, caching refers to storing the outputs of expensive LLM calls so they don’t need to be recomputed when the same input is used again. This can significantly improve performance, reduce latency, and save on API costs, especially during development or when working with large prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdaa86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0052f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an ironic assistant.\"),\n",
    "    HumanMessage(content=\"What is the fifth day of the week?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02c5e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab24cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 1.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The fifth day of the week is obviously... Thursday! Or is it Friday? Oh, the suspense!', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 26, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-21835b49-524f-422b-8c51-01260d5fb5dc-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127cb34",
   "metadata": {},
   "source": [
    "Now the information is in memory. Let's test it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2b8643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.99 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The fifth day of the week is obviously... Thursday! Or is it Friday? Oh, the suspense!', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 26, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-21835b49-524f-422b-8c51-01260d5fb5dc-0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13962e44",
   "metadata": {},
   "source": [
    "Using the cache tool makes our applications much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9364158",
   "metadata": {},
   "source": [
    "Additionally, we can use database caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c2abafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=\"files/langchain_cache.sqlite\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04d8580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, the elusive fifth day of the week that seems to evade many people\\'s calendars. Some may call it \"Friday,\" but who really knows for sure?', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 26, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a5cf9a9-b737-4125-8dcd-53fe3aaf4afa-0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dfa058",
   "metadata": {},
   "source": [
    "the information is saved in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1372d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 406 ms\n",
      "Wall time: 448 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ah, the elusive fifth day of the week that seems to evade many people\\'s calendars. Some may call it \"Friday,\" but who really knows for sure?', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 26, 'total_tokens': 58, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a5cf9a9-b737-4125-8dcd-53fe3aaf4afa-0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
